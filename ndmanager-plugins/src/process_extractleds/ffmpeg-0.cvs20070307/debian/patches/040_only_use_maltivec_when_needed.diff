Index: ffmpeg-0.cvs20070307/configure
===================================================================
--- ffmpeg-0.cvs20070307.orig/configure	2007-03-28 16:07:04.000000000 +0200
+++ ffmpeg-0.cvs20070307/configure	2007-03-28 16:17:39.000000000 +0200
@@ -1410,11 +1410,13 @@
         if test -n "`$cc -v 2>&1 | grep version | grep Apple`"; then
             add_cflags "-faltivec"
         else
-            add_cflags "-maltivec"
+            ALTIVECFLAGS="$ALTIVECFLAGS -maltivec"
         fi
     fi
 fi
 
+save_flags
+temp_cflags $ALTIVECFLAGS
 check_header altivec.h
 
 # check if our compiler supports Motorola AltiVec C API
@@ -1461,6 +1463,7 @@
         }
 EOF
 fi
+restore_flags
 
 # mmi only available on mips
 if test $mmi = "default"; then
@@ -1862,6 +1865,7 @@
 fi
 
 echo "OPTFLAGS=$CFLAGS" >> config.mak
+echo "ALTIVECFLAGS=$ALTIVECFLAGS" >> config.mak
 echo "VHOOKCFLAGS=$VHOOKCFLAGS">>config.mak
 echo "LDFLAGS=$LDFLAGS" >> config.mak
 echo "LDCONFIG=$LDCONFIG" >> config.mak
Index: ffmpeg-0.cvs20070307/libavcodec/Makefile
===================================================================
--- ffmpeg-0.cvs20070307.orig/libavcodec/Makefile	2007-03-28 16:17:35.000000000 +0200
+++ ffmpeg-0.cvs20070307/libavcodec/Makefile	2007-03-28 16:17:59.000000000 +0200
@@ -310,7 +310,8 @@
 
 OBJS-$(HAVE_XVMC_ACCEL)                += xvmcvideo.o
 
-OBJS += imgresample.o
+OBJS += imgresample.o imgresample_altivec.o
+imgresample_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
 
 # i386 mmx specific stuff
 ifeq ($(TARGET_MMX),yes)
@@ -385,11 +386,22 @@
                                           ppc/fdct_altivec.o         \
                                           ppc/float_altivec.o        \
 
+ppc/dsputil_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/mpegvideo_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/idct_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/fft_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/gmc_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/fdct_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/float_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+
 ifeq ($(TARGET_ALTIVEC),yes)
 OBJS-$(CONFIG_H264_DECODER)            += ppc/h264_altivec.o
 OBJS-$(CONFIG_SNOW_DECODER)            += ppc/snow_altivec.o
 OBJS-$(CONFIG_VC1_DECODER)             += ppc/vc1dsp_altivec.o
 OBJS-$(CONFIG_WMV3_DECODER)            += ppc/vc1dsp_altivec.o
+ppc/h264_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/snow_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+ppc/vc1dsp_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
 endif
 
 OBJS-$(TARGET_ARCH_BFIN)               += bfin/dsputil_bfin.o \
Index: ffmpeg-0.cvs20070307/libavcodec/dsputil.h
===================================================================
--- ffmpeg-0.cvs20070307.orig/libavcodec/dsputil.h	2007-03-07 14:37:04.000000000 +0100
+++ ffmpeg-0.cvs20070307/libavcodec/dsputil.h	2007-03-28 16:17:39.000000000 +0200
@@ -556,12 +556,6 @@
 
 extern int mm_flags;
 
-#if defined(HAVE_ALTIVEC) && !defined(CONFIG_DARWIN)
-#define pixel altivec_pixel
-#include <altivec.h>
-#undef pixel
-#endif
-
 #define DECLARE_ALIGNED_8(t,v)    t v __attribute__ ((aligned (16)))
 #define STRIDE_ALIGN 16
 
Index: ffmpeg-0.cvs20070307/libavcodec/imgresample.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libavcodec/imgresample.c	2007-03-07 14:37:04.000000000 +0100
+++ ffmpeg-0.cvs20070307/libavcodec/imgresample.c	2007-03-28 16:17:39.000000000 +0200
@@ -284,133 +284,6 @@
 }
 #endif
 
-#ifdef HAVE_ALTIVEC
-typedef         union {
-    vector unsigned char v;
-    unsigned char c[16];
-} vec_uc_t;
-
-typedef         union {
-    vector signed short v;
-    signed short s[8];
-} vec_ss_t;
-
-void v_resample16_altivec(uint8_t *dst, int dst_width, const uint8_t *src,
-                          int wrap, int16_t *filter)
-{
-    int sum, i;
-    const uint8_t *s;
-    vector unsigned char *tv, tmp, dstv, zero;
-    vec_ss_t srchv[4], srclv[4], fv[4];
-    vector signed short zeros, sumhv, sumlv;
-    s = src;
-
-    for(i=0;i<4;i++)
-    {
-        /*
-           The vec_madds later on does an implicit >>15 on the result.
-           Since FILTER_BITS is 8, and we have 15 bits of magnitude in
-           a signed short, we have just enough bits to pre-shift our
-           filter constants <<7 to compensate for vec_madds.
-        */
-        fv[i].s[0] = filter[i] << (15-FILTER_BITS);
-        fv[i].v = vec_splat(fv[i].v, 0);
-    }
-
-    zero = vec_splat_u8(0);
-    zeros = vec_splat_s16(0);
-
-
-    /*
-       When we're resampling, we'd ideally like both our input buffers,
-       and output buffers to be 16-byte aligned, so we can do both aligned
-       reads and writes. Sadly we can't always have this at the moment, so
-       we opt for aligned writes, as unaligned writes have a huge overhead.
-       To do this, do enough scalar resamples to get dst 16-byte aligned.
-    */
-    i = (-(int)dst) & 0xf;
-    while(i>0) {
-        sum = s[0 * wrap] * filter[0] +
-        s[1 * wrap] * filter[1] +
-        s[2 * wrap] * filter[2] +
-        s[3 * wrap] * filter[3];
-        sum = sum >> FILTER_BITS;
-        if (sum<0) sum = 0; else if (sum>255) sum=255;
-        dst[0] = sum;
-        dst++;
-        s++;
-        dst_width--;
-        i--;
-    }
-
-    /* Do our altivec resampling on 16 pixels at once. */
-    while(dst_width>=16) {
-        /*
-           Read 16 (potentially unaligned) bytes from each of
-           4 lines into 4 vectors, and split them into shorts.
-           Interleave the multipy/accumulate for the resample
-           filter with the loads to hide the 3 cycle latency
-           the vec_madds have.
-        */
-        tv = (vector unsigned char *) &s[0 * wrap];
-        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[i * wrap]));
-        srchv[0].v = (vector signed short) vec_mergeh(zero, tmp);
-        srclv[0].v = (vector signed short) vec_mergel(zero, tmp);
-        sumhv = vec_madds(srchv[0].v, fv[0].v, zeros);
-        sumlv = vec_madds(srclv[0].v, fv[0].v, zeros);
-
-        tv = (vector unsigned char *) &s[1 * wrap];
-        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[1 * wrap]));
-        srchv[1].v = (vector signed short) vec_mergeh(zero, tmp);
-        srclv[1].v = (vector signed short) vec_mergel(zero, tmp);
-        sumhv = vec_madds(srchv[1].v, fv[1].v, sumhv);
-        sumlv = vec_madds(srclv[1].v, fv[1].v, sumlv);
-
-        tv = (vector unsigned char *) &s[2 * wrap];
-        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[2 * wrap]));
-        srchv[2].v = (vector signed short) vec_mergeh(zero, tmp);
-        srclv[2].v = (vector signed short) vec_mergel(zero, tmp);
-        sumhv = vec_madds(srchv[2].v, fv[2].v, sumhv);
-        sumlv = vec_madds(srclv[2].v, fv[2].v, sumlv);
-
-        tv = (vector unsigned char *) &s[3 * wrap];
-        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[3 * wrap]));
-        srchv[3].v = (vector signed short) vec_mergeh(zero, tmp);
-        srclv[3].v = (vector signed short) vec_mergel(zero, tmp);
-        sumhv = vec_madds(srchv[3].v, fv[3].v, sumhv);
-        sumlv = vec_madds(srclv[3].v, fv[3].v, sumlv);
-
-        /*
-           Pack the results into our destination vector,
-           and do an aligned write of that back to memory.
-        */
-        dstv = vec_packsu(sumhv, sumlv) ;
-        vec_st(dstv, 0, (vector unsigned char *) dst);
-
-        dst+=16;
-        s+=16;
-        dst_width-=16;
-    }
-
-    /*
-       If there are any leftover pixels, resample them
-       with the slow scalar method.
-    */
-    while(dst_width>0) {
-        sum = s[0 * wrap] * filter[0] +
-        s[1 * wrap] * filter[1] +
-        s[2 * wrap] * filter[2] +
-        s[3 * wrap] * filter[3];
-        sum = sum >> FILTER_BITS;
-        if (sum<0) sum = 0; else if (sum>255) sum=255;
-        dst[0] = sum;
-        dst++;
-        s++;
-        dst_width--;
-    }
-}
-#endif
-
 /* slow version to handle limit cases. Does not need optimisation */
 static void h_resample_slow(uint8_t *dst, int dst_width,
                             const uint8_t *src, int src_width,
Index: ffmpeg-0.cvs20070307/libavcodec/imgresample_altivec.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ ffmpeg-0.cvs20070307/libavcodec/imgresample_altivec.c	2007-03-28 16:17:39.000000000 +0200
@@ -0,0 +1,164 @@
+/*
+ * High quality image resampling with polyphase filters
+ * Copyright (c) 2001 Fabrice Bellard.
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file imgresample_altivec.c
+ * High quality image resampling with polyphase filters, AltiVec version.
+ */
+
+#include "avcodec.h"
+#include "swscale.h"
+#include "dsputil.h"
+
+#if defined(HAVE_ALTIVEC) && !defined(CONFIG_DARWIN)
+#define pixel altivec_pixel
+#include <altivec.h>
+#undef pixel
+#endif
+
+/* 6 bits precision is needed for MMX */
+#define FILTER_BITS   8
+
+#ifdef HAVE_ALTIVEC
+typedef         union {
+    vector unsigned char v;
+    unsigned char c[16];
+} vec_uc_t;
+
+typedef         union {
+    vector signed short v;
+    signed short s[8];
+} vec_ss_t;
+
+void v_resample16_altivec(uint8_t *dst, int dst_width, const uint8_t *src,
+                          int wrap, int16_t *filter)
+{
+    int sum, i;
+    const uint8_t *s;
+    vector unsigned char *tv, tmp, dstv, zero;
+    vec_ss_t srchv[4], srclv[4], fv[4];
+    vector signed short zeros, sumhv, sumlv;
+    s = src;
+
+    for(i=0;i<4;i++)
+    {
+        /*
+           The vec_madds later on does an implicit >>15 on the result.
+           Since FILTER_BITS is 8, and we have 15 bits of magnitude in
+           a signed short, we have just enough bits to pre-shift our
+           filter constants <<7 to compensate for vec_madds.
+        */
+        fv[i].s[0] = filter[i] << (15-FILTER_BITS);
+        fv[i].v = vec_splat(fv[i].v, 0);
+    }
+
+    zero = vec_splat_u8(0);
+    zeros = vec_splat_s16(0);
+
+
+    /*
+       When we're resampling, we'd ideally like both our input buffers,
+       and output buffers to be 16-byte aligned, so we can do both aligned
+       reads and writes. Sadly we can't always have this at the moment, so
+       we opt for aligned writes, as unaligned writes have a huge overhead.
+       To do this, do enough scalar resamples to get dst 16-byte aligned.
+    */
+    i = (-(int)dst) & 0xf;
+    while(i>0) {
+        sum = s[0 * wrap] * filter[0] +
+        s[1 * wrap] * filter[1] +
+        s[2 * wrap] * filter[2] +
+        s[3 * wrap] * filter[3];
+        sum = sum >> FILTER_BITS;
+        if (sum<0) sum = 0; else if (sum>255) sum=255;
+        dst[0] = sum;
+        dst++;
+        s++;
+        dst_width--;
+        i--;
+    }
+
+    /* Do our altivec resampling on 16 pixels at once. */
+    while(dst_width>=16) {
+        /*
+           Read 16 (potentially unaligned) bytes from each of
+           4 lines into 4 vectors, and split them into shorts.
+           Interleave the multipy/accumulate for the resample
+           filter with the loads to hide the 3 cycle latency
+           the vec_madds have.
+        */
+        tv = (vector unsigned char *) &s[0 * wrap];
+        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[i * wrap]));
+        srchv[0].v = (vector signed short) vec_mergeh(zero, tmp);
+        srclv[0].v = (vector signed short) vec_mergel(zero, tmp);
+        sumhv = vec_madds(srchv[0].v, fv[0].v, zeros);
+        sumlv = vec_madds(srclv[0].v, fv[0].v, zeros);
+
+        tv = (vector unsigned char *) &s[1 * wrap];
+        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[1 * wrap]));
+        srchv[1].v = (vector signed short) vec_mergeh(zero, tmp);
+        srclv[1].v = (vector signed short) vec_mergel(zero, tmp);
+        sumhv = vec_madds(srchv[1].v, fv[1].v, sumhv);
+        sumlv = vec_madds(srclv[1].v, fv[1].v, sumlv);
+
+        tv = (vector unsigned char *) &s[2 * wrap];
+        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[2 * wrap]));
+        srchv[2].v = (vector signed short) vec_mergeh(zero, tmp);
+        srclv[2].v = (vector signed short) vec_mergel(zero, tmp);
+        sumhv = vec_madds(srchv[2].v, fv[2].v, sumhv);
+        sumlv = vec_madds(srclv[2].v, fv[2].v, sumlv);
+
+        tv = (vector unsigned char *) &s[3 * wrap];
+        tmp = vec_perm(tv[0], tv[1], vec_lvsl(0, &s[3 * wrap]));
+        srchv[3].v = (vector signed short) vec_mergeh(zero, tmp);
+        srclv[3].v = (vector signed short) vec_mergel(zero, tmp);
+        sumhv = vec_madds(srchv[3].v, fv[3].v, sumhv);
+        sumlv = vec_madds(srclv[3].v, fv[3].v, sumlv);
+
+        /*
+           Pack the results into our destination vector,
+           and do an aligned write of that back to memory.
+        */
+        dstv = vec_packsu(sumhv, sumlv) ;
+        vec_st(dstv, 0, (vector unsigned char *) dst);
+
+        dst+=16;
+        s+=16;
+        dst_width-=16;
+    }
+
+    /*
+       If there are any leftover pixels, resample them
+       with the slow scalar method.
+    */
+    while(dst_width>0) {
+        sum = s[0 * wrap] * filter[0] +
+        s[1 * wrap] * filter[1] +
+        s[2 * wrap] * filter[2] +
+        s[3 * wrap] * filter[3];
+        sum = sum >> FILTER_BITS;
+        if (sum<0) sum = 0; else if (sum>255) sum=255;
+        dst[0] = sum;
+        dst++;
+        s++;
+        dst_width--;
+    }
+}
+#endif
+
Index: ffmpeg-0.cvs20070307/libpostproc/Makefile
===================================================================
--- ffmpeg-0.cvs20070307.orig/libpostproc/Makefile	2007-03-07 14:37:04.000000000 +0100
+++ ffmpeg-0.cvs20070307/libpostproc/Makefile	2007-03-28 16:17:39.000000000 +0200
@@ -10,8 +10,10 @@
 LIBVERSION=$(SPPVERSION)
 LIBMAJOR=$(SPPMAJOR)
 
-STATIC_OBJS=postprocess.o
-SHARED_OBJS=postprocess_pic.o
+STATIC_OBJS=postprocess.o postprocess_altivec.o
+SHARED_OBJS=postprocess_pic.o postprocess_altivec_pic.o
+postprocess_altivec.o: CFLAGS+= $(ALTIVECFLAGS)
+postprocess_altivec_pic.o: CFLAGS+= $(ALTIVECFLAGS)
 
 HEADERS = postprocess.h
 
@@ -21,4 +23,6 @@
 
 postprocess_pic.o: postprocess.c
 	$(CC) -c $(CFLAGS) -fomit-frame-pointer -fPIC -DPIC -o $@ $<
+postprocess_altivec_pic.o: postprocess_altivec.c
+	$(CC) -c $(CFLAGS) -fomit-frame-pointer -fPIC -DPIC -o $@ $<
 
Index: ffmpeg-0.cvs20070307/libpostproc/postprocess.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libpostproc/postprocess.c	2007-03-07 11:17:32.000000000 +0100
+++ ffmpeg-0.cvs20070307/libpostproc/postprocess.c	2007-03-28 16:17:39.000000000 +0200
@@ -95,10 +95,6 @@
 
 #include "mangle.h" //FIXME should be supressed
 
-#ifdef HAVE_ALTIVEC_H
-#include <altivec.h>
-#endif
-
 #define GET_MODE_BUFFER_SIZE 500
 #define OPTIONS_ARRAY_SIZE 10
 #define BLOCK_SIZE 8
@@ -614,8 +610,9 @@
 #undef RENAME
 #define HAVE_ALTIVEC
 #define RENAME(a) a ## _altivec
-#include "postprocess_altivec_template.c"
-#include "postprocess_template.c"
+//#include "postprocess_altivec_template.c"
+//#include "postprocess_template.c"
+void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height, QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c);
 #endif
 #endif //ARCH_POWERPC
 
Index: ffmpeg-0.cvs20070307/libpostproc/postprocess_altivec.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ ffmpeg-0.cvs20070307/libpostproc/postprocess_altivec.c	2007-03-28 16:17:39.000000000 +0200
@@ -0,0 +1,614 @@
+/*
+    Copyright (C) 2001-2003 Michael Niedermayer (michaelni@gmx.at)
+
+    AltiVec optimizations (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+/**
+ * @file postprocess.c
+ * postprocessing.
+ */
+
+/*
+                        C       MMX     MMX2    3DNow   AltiVec
+isVertDC                Ec      Ec                      Ec
+isVertMinMaxOk          Ec      Ec                      Ec
+doVertLowPass           E               e       e       Ec
+doVertDefFilter         Ec      Ec      e       e       Ec
+isHorizDC               Ec      Ec                      Ec
+isHorizMinMaxOk         a       E                       Ec
+doHorizLowPass          E               e       e       Ec
+doHorizDefFilter        Ec      Ec      e       e       Ec
+do_a_deblock            Ec      E       Ec      E
+deRing                  E               e       e*      Ecp
+Vertical RKAlgo1        E               a       a
+Horizontal RKAlgo1                      a       a
+Vertical X1#            a               E       E
+Horizontal X1#          a               E       E
+LinIpolDeinterlace      e               E       E*
+CubicIpolDeinterlace    a               e       e*
+LinBlendDeinterlace     e               E       E*
+MedianDeinterlace#      E       Ec      Ec
+TempDeNoiser#           E               e       e       Ec
+
+* i dont have a 3dnow CPU -> its untested, but noone said it doesnt work so it seems to work
+# more or less selfinvented filters so the exactness isnt too meaningfull
+E = Exact implementation
+e = allmost exact implementation (slightly different rounding,...)
+a = alternative / approximate impl
+c = checked against the other implementations (-vo md5)
+p = partially optimized, still some work to do
+*/
+
+/*
+TODO:
+reduce the time wasted on the mem transfer
+unroll stuff if instructions depend too much on the prior one
+move YScale thing to the end instead of fixing QP
+write a faster and higher quality deblocking filter :)
+make the mainloop more flexible (variable number of blocks at once
+        (the if/else stuff per block is slowing things down)
+compare the quality & speed of all filters
+split this huge file
+optimize c versions
+try to unroll inner for(x=0 ... loop to avoid these damn if(x ... checks
+...
+*/
+
+//Changelog: use the Subversion log
+
+#include "config.h"
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#ifdef HAVE_MALLOC_H
+#include <malloc.h>
+#endif
+//#undef HAVE_MMX2
+//#define HAVE_3DNOW
+//#undef HAVE_MMX
+//#undef ARCH_X86
+//#define DEBUG_BRIGHTNESS
+#ifdef USE_FASTMEMCPY
+#include "libvo/fastmemcpy.h"
+#endif
+#include "postprocess.h"
+#include "postprocess_internal.h"
+
+#include "mangle.h" //FIXME should be supressed
+
+#ifdef HAVE_ALTIVEC_H
+#include <altivec.h>
+#endif
+
+#ifndef HAVE_MEMALIGN
+#define memalign(a,b) malloc(b)
+#endif
+
+#define MIN(a,b) ((a) > (b) ? (b) : (a))
+#define MAX(a,b) ((a) < (b) ? (b) : (a))
+#define ABS(a) ((a) > 0 ? (a) : (-(a)))
+#define SIGN(a) ((a) > 0 ? 1 : -1)
+
+#define GET_MODE_BUFFER_SIZE 500
+#define OPTIONS_ARRAY_SIZE 10
+#define BLOCK_SIZE 8
+#define TEMP_STRIDE 8
+//#define NUM_BLOCKS_AT_ONCE 16 //not used yet
+
+#if defined(__GNUC__) && (__GNUC__ > 3 || __GNUC__ == 3 && __GNUC_MINOR__ > 0)
+#    define attribute_used __attribute__((used))
+#    define always_inline __attribute__((always_inline)) inline
+#else
+#    define attribute_used
+#    define always_inline inline
+#endif
+
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
+static uint64_t __attribute__((aligned(8))) attribute_used w05= 0x0005000500050005LL;
+static uint64_t __attribute__((aligned(8))) attribute_used w04= 0x0004000400040004LL;
+static uint64_t __attribute__((aligned(8))) attribute_used w20= 0x0020002000200020LL;
+static uint64_t __attribute__((aligned(8))) attribute_used b00= 0x0000000000000000LL;
+static uint64_t __attribute__((aligned(8))) attribute_used b01= 0x0101010101010101LL;
+static uint64_t __attribute__((aligned(8))) attribute_used b02= 0x0202020202020202LL;
+static uint64_t __attribute__((aligned(8))) attribute_used b08= 0x0808080808080808LL;
+static uint64_t __attribute__((aligned(8))) attribute_used b80= 0x8080808080808080LL;
+#endif
+
+static uint8_t clip_table[3*256];
+static uint8_t * const clip_tab= clip_table + 256;
+
+static const int verbose= 0;
+
+static const int attribute_used deringThreshold= 20;
+
+
+static struct PPFilter filters[]=
+{
+        {"hb", "hdeblock",              1, 1, 3, H_DEBLOCK},
+        {"vb", "vdeblock",              1, 2, 4, V_DEBLOCK},
+/*      {"hr", "rkhdeblock",            1, 1, 3, H_RK1_FILTER},
+        {"vr", "rkvdeblock",            1, 2, 4, V_RK1_FILTER},*/
+        {"h1", "x1hdeblock",            1, 1, 3, H_X1_FILTER},
+        {"v1", "x1vdeblock",            1, 2, 4, V_X1_FILTER},
+        {"ha", "ahdeblock",             1, 1, 3, H_A_DEBLOCK},
+        {"va", "avdeblock",             1, 2, 4, V_A_DEBLOCK},
+        {"dr", "dering",                1, 5, 6, DERING},
+        {"al", "autolevels",            0, 1, 2, LEVEL_FIX},
+        {"lb", "linblenddeint",         1, 1, 4, LINEAR_BLEND_DEINT_FILTER},
+        {"li", "linipoldeint",          1, 1, 4, LINEAR_IPOL_DEINT_FILTER},
+        {"ci", "cubicipoldeint",        1, 1, 4, CUBIC_IPOL_DEINT_FILTER},
+        {"md", "mediandeint",           1, 1, 4, MEDIAN_DEINT_FILTER},
+        {"fd", "ffmpegdeint",           1, 1, 4, FFMPEG_DEINT_FILTER},
+        {"l5", "lowpass5",              1, 1, 4, LOWPASS5_DEINT_FILTER},
+        {"tn", "tmpnoise",              1, 7, 8, TEMP_NOISE_FILTER},
+        {"fq", "forcequant",            1, 0, 0, FORCE_QUANT},
+        {NULL, NULL,0,0,0,0} //End Marker
+};
+
+static const char *replaceTable[]=
+{
+        "default",      "hdeblock:a,vdeblock:a,dering:a",
+        "de",           "hdeblock:a,vdeblock:a,dering:a",
+        "fast",         "x1hdeblock:a,x1vdeblock:a,dering:a",
+        "fa",           "x1hdeblock:a,x1vdeblock:a,dering:a",
+        "ac",           "ha:a:128:7,va:a,dering:a",
+        NULL //End Marker
+};
+
+
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
+static inline void prefetchnta(void *p)
+{
+        asm volatile(   "prefetchnta (%0)\n\t"
+                : : "r" (p)
+        );
+}
+
+static inline void prefetcht0(void *p)
+{
+        asm volatile(   "prefetcht0 (%0)\n\t"
+                : : "r" (p)
+        );
+}
+
+static inline void prefetcht1(void *p)
+{
+        asm volatile(   "prefetcht1 (%0)\n\t"
+                : : "r" (p)
+        );
+}
+
+static inline void prefetcht2(void *p)
+{
+        asm volatile(   "prefetcht2 (%0)\n\t"
+                : : "r" (p)
+        );
+}
+#endif
+
+// The horizontal Functions exist only in C cuz the MMX code is faster with vertical filters and transposing
+
+/**
+ * Check if the given 8x8 Block is mostly "flat"
+ */
+static inline int isHorizDC_C(uint8_t src[], int stride, PPContext *c)
+{
+        int numEq= 0;
+        int y;
+        const int dcOffset= ((c->nonBQP*c->ppMode.baseDcDiff)>>8) + 1;
+        const int dcThreshold= dcOffset*2 + 1;
+
+        for(y=0; y<BLOCK_SIZE; y++)
+        {
+                if(((unsigned)(src[0] - src[1] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[1] - src[2] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[2] - src[3] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[3] - src[4] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[4] - src[5] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[5] - src[6] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[6] - src[7] + dcOffset)) < dcThreshold) numEq++;
+                src+= stride;
+        }
+        return numEq > c->ppMode.flatnessThreshold;
+}
+
+/**
+ * Check if the middle 8x8 Block in the given 8x16 block is flat
+ */
+static inline int isVertDC_C(uint8_t src[], int stride, PPContext *c){
+        int numEq= 0;
+        int y;
+        const int dcOffset= ((c->nonBQP*c->ppMode.baseDcDiff)>>8) + 1;
+        const int dcThreshold= dcOffset*2 + 1;
+
+        src+= stride*4; // src points to begin of the 8x8 Block
+        for(y=0; y<BLOCK_SIZE-1; y++)
+        {
+                if(((unsigned)(src[0] - src[0+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[1] - src[1+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[2] - src[2+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[3] - src[3+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[4] - src[4+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[5] - src[5+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[6] - src[6+stride] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[7] - src[7+stride] + dcOffset)) < dcThreshold) numEq++;
+                src+= stride;
+        }
+        return numEq > c->ppMode.flatnessThreshold;
+}
+
+static inline int isHorizMinMaxOk_C(uint8_t src[], int stride, int QP)
+{
+        int i;
+#if 1
+        for(i=0; i<2; i++){
+                if((unsigned)(src[0] - src[5] + 2*QP) > 4*QP) return 0;
+                src += stride;
+                if((unsigned)(src[2] - src[7] + 2*QP) > 4*QP) return 0;
+                src += stride;
+                if((unsigned)(src[4] - src[1] + 2*QP) > 4*QP) return 0;
+                src += stride;
+                if((unsigned)(src[6] - src[3] + 2*QP) > 4*QP) return 0;
+                src += stride;
+        }
+#else
+        for(i=0; i<8; i++){
+                if((unsigned)(src[0] - src[7] + 2*QP) > 4*QP) return 0;
+                src += stride;
+        }
+#endif
+        return 1;
+}
+
+static inline int isVertMinMaxOk_C(uint8_t src[], int stride, int QP)
+{
+#if 1
+#if 1
+        int x;
+        src+= stride*4;
+        for(x=0; x<BLOCK_SIZE; x+=4)
+        {
+                if((unsigned)(src[  x + 0*stride] - src[  x + 5*stride] + 2*QP) > 4*QP) return 0;
+                if((unsigned)(src[1+x + 2*stride] - src[1+x + 7*stride] + 2*QP) > 4*QP) return 0;
+                if((unsigned)(src[2+x + 4*stride] - src[2+x + 1*stride] + 2*QP) > 4*QP) return 0;
+                if((unsigned)(src[3+x + 6*stride] - src[3+x + 3*stride] + 2*QP) > 4*QP) return 0;
+        }
+#else
+        int x;
+        src+= stride*3;
+        for(x=0; x<BLOCK_SIZE; x++)
+        {
+                if((unsigned)(src[x + stride] - src[x + (stride<<3)] + 2*QP) > 4*QP) return 0;
+        }
+#endif
+        return 1;
+#else
+        int x;
+        src+= stride*4;
+        for(x=0; x<BLOCK_SIZE; x++)
+        {
+                int min=255;
+                int max=0;
+                int y;
+                for(y=0; y<8; y++){
+                        int v= src[x + y*stride];
+                        if(v>max) max=v;
+                        if(v<min) min=v;
+                }
+                if(max-min > 2*QP) return 0;
+        }
+        return 1;
+#endif
+}
+
+static inline int horizClassify_C(uint8_t src[], int stride, PPContext *c){
+        if( isHorizDC_C(src, stride, c) ){
+                if( isHorizMinMaxOk_C(src, stride, c->QP) )
+                        return 1;
+                else
+                        return 0;
+        }else{
+                return 2;
+        }
+}
+
+static inline int vertClassify_C(uint8_t src[], int stride, PPContext *c){
+        if( isVertDC_C(src, stride, c) ){
+                if( isVertMinMaxOk_C(src, stride, c->QP) )
+                        return 1;
+                else
+                        return 0;
+        }else{
+                return 2;
+        }
+}
+
+static inline void doHorizDefFilter_C(uint8_t dst[], int stride, PPContext *c)
+{
+        int y;
+        for(y=0; y<BLOCK_SIZE; y++)
+        {
+                const int middleEnergy= 5*(dst[4] - dst[3]) + 2*(dst[2] - dst[5]);
+
+                if(ABS(middleEnergy) < 8*c->QP)
+                {
+                        const int q=(dst[3] - dst[4])/2;
+                        const int leftEnergy=  5*(dst[2] - dst[1]) + 2*(dst[0] - dst[3]);
+                        const int rightEnergy= 5*(dst[6] - dst[5]) + 2*(dst[4] - dst[7]);
+
+                        int d= ABS(middleEnergy) - MIN( ABS(leftEnergy), ABS(rightEnergy) );
+                        d= MAX(d, 0);
+
+                        d= (5*d + 32) >> 6;
+                        d*= SIGN(-middleEnergy);
+
+                        if(q>0)
+                        {
+                                d= d<0 ? 0 : d;
+                                d= d>q ? q : d;
+                        }
+                        else
+                        {
+                                d= d>0 ? 0 : d;
+                                d= d<q ? q : d;
+                        }
+
+                        dst[3]-= d;
+                        dst[4]+= d;
+                }
+                dst+= stride;
+        }
+}
+
+/**
+ * Do a horizontal low pass filter on the 10x8 block (dst points to middle 8x8 Block)
+ * using the 9-Tap Filter (1,1,2,2,4,2,2,1,1)/16 (C version)
+ */
+static inline void doHorizLowPass_C(uint8_t dst[], int stride, PPContext *c)
+{
+        int y;
+        for(y=0; y<BLOCK_SIZE; y++)
+        {
+                const int first= ABS(dst[-1] - dst[0]) < c->QP ? dst[-1] : dst[0];
+                const int last= ABS(dst[8] - dst[7]) < c->QP ? dst[8] : dst[7];
+
+                int sums[10];
+                sums[0] = 4*first + dst[0] + dst[1] + dst[2] + 4;
+                sums[1] = sums[0] - first  + dst[3];
+                sums[2] = sums[1] - first  + dst[4];
+                sums[3] = sums[2] - first  + dst[5];
+                sums[4] = sums[3] - first  + dst[6];
+                sums[5] = sums[4] - dst[0] + dst[7];
+                sums[6] = sums[5] - dst[1] + last;
+                sums[7] = sums[6] - dst[2] + last;
+                sums[8] = sums[7] - dst[3] + last;
+                sums[9] = sums[8] - dst[4] + last;
+
+                dst[0]= (sums[0] + sums[2] + 2*dst[0])>>4;
+                dst[1]= (sums[1] + sums[3] + 2*dst[1])>>4;
+                dst[2]= (sums[2] + sums[4] + 2*dst[2])>>4;
+                dst[3]= (sums[3] + sums[5] + 2*dst[3])>>4;
+                dst[4]= (sums[4] + sums[6] + 2*dst[4])>>4;
+                dst[5]= (sums[5] + sums[7] + 2*dst[5])>>4;
+                dst[6]= (sums[6] + sums[8] + 2*dst[6])>>4;
+                dst[7]= (sums[7] + sums[9] + 2*dst[7])>>4;
+
+                dst+= stride;
+        }
+}
+
+/**
+ * Experimental Filter 1 (Horizontal)
+ * will not damage linear gradients
+ * Flat blocks should look like they where passed through the (1,1,2,2,4,2,2,1,1) 9-Tap filter
+ * can only smooth blocks at the expected locations (it cant smooth them if they did move)
+ * MMX2 version does correct clipping C version doesnt
+ * not identical with the vertical one
+ */
+static inline void horizX1Filter(uint8_t *src, int stride, int QP)
+{
+        int y;
+        static uint64_t *lut= NULL;
+        if(lut==NULL)
+        {
+                int i;
+                lut= (uint64_t*)memalign(8, 256*8);
+                for(i=0; i<256; i++)
+                {
+                        int v= i < 128 ? 2*i : 2*(i-256);
+/*
+//Simulate 112242211 9-Tap filter
+                        uint64_t a= (v/16) & 0xFF;
+                        uint64_t b= (v/8) & 0xFF;
+                        uint64_t c= (v/4) & 0xFF;
+                        uint64_t d= (3*v/8) & 0xFF;
+*/
+//Simulate piecewise linear interpolation
+                        uint64_t a= (v/16) & 0xFF;
+                        uint64_t b= (v*3/16) & 0xFF;
+                        uint64_t c= (v*5/16) & 0xFF;
+                        uint64_t d= (7*v/16) & 0xFF;
+                        uint64_t A= (0x100 - a)&0xFF;
+                        uint64_t B= (0x100 - b)&0xFF;
+                        uint64_t C= (0x100 - c)&0xFF;
+                        uint64_t D= (0x100 - c)&0xFF;
+
+                        lut[i]   = (a<<56) | (b<<48) | (c<<40) | (d<<32) |
+                                (D<<24) | (C<<16) | (B<<8) | (A);
+                        //lut[i] = (v<<32) | (v<<24);
+                }
+        }
+
+        for(y=0; y<BLOCK_SIZE; y++)
+        {
+                int a= src[1] - src[2];
+                int b= src[3] - src[4];
+                int c= src[5] - src[6];
+
+                int d= MAX(ABS(b) - (ABS(a) + ABS(c))/2, 0);
+
+                if(d < QP)
+                {
+                        int v = d * SIGN(-b);
+
+                        src[1] +=v/8;
+                        src[2] +=v/4;
+                        src[3] +=3*v/8;
+                        src[4] -=3*v/8;
+                        src[5] -=v/4;
+                        src[6] -=v/8;
+
+                }
+                src+=stride;
+        }
+}
+
+/**
+ * accurate deblock filter
+ */
+static always_inline void do_a_deblock_C(uint8_t *src, int step, int stride, PPContext *c){
+        int y;
+        const int QP= c->QP;
+        const int dcOffset= ((c->nonBQP*c->ppMode.baseDcDiff)>>8) + 1;
+        const int dcThreshold= dcOffset*2 + 1;
+//START_TIMER
+        src+= step*4; // src points to begin of the 8x8 Block
+        for(y=0; y<8; y++){
+                int numEq= 0;
+
+                if(((unsigned)(src[-1*step] - src[0*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 0*step] - src[1*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 1*step] - src[2*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 2*step] - src[3*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 3*step] - src[4*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 4*step] - src[5*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 5*step] - src[6*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 6*step] - src[7*step] + dcOffset)) < dcThreshold) numEq++;
+                if(((unsigned)(src[ 7*step] - src[8*step] + dcOffset)) < dcThreshold) numEq++;
+                if(numEq > c->ppMode.flatnessThreshold){
+                        int min, max, x;
+
+                        if(src[0] > src[step]){
+                            max= src[0];
+                            min= src[step];
+                        }else{
+                            max= src[step];
+                            min= src[0];
+                        }
+                        for(x=2; x<8; x+=2){
+                                if(src[x*step] > src[(x+1)*step]){
+                                        if(src[x    *step] > max) max= src[ x   *step];
+                                        if(src[(x+1)*step] < min) min= src[(x+1)*step];
+                                }else{
+                                        if(src[(x+1)*step] > max) max= src[(x+1)*step];
+                                        if(src[ x   *step] < min) min= src[ x   *step];
+                                }
+                        }
+                        if(max-min < 2*QP){
+                                const int first= ABS(src[-1*step] - src[0]) < QP ? src[-1*step] : src[0];
+                                const int last= ABS(src[8*step] - src[7*step]) < QP ? src[8*step] : src[7*step];
+
+                                int sums[10];
+                                sums[0] = 4*first + src[0*step] + src[1*step] + src[2*step] + 4;
+                                sums[1] = sums[0] - first       + src[3*step];
+                                sums[2] = sums[1] - first       + src[4*step];
+                                sums[3] = sums[2] - first       + src[5*step];
+                                sums[4] = sums[3] - first       + src[6*step];
+                                sums[5] = sums[4] - src[0*step] + src[7*step];
+                                sums[6] = sums[5] - src[1*step] + last;
+                                sums[7] = sums[6] - src[2*step] + last;
+                                sums[8] = sums[7] - src[3*step] + last;
+                                sums[9] = sums[8] - src[4*step] + last;
+
+                                src[0*step]= (sums[0] + sums[2] + 2*src[0*step])>>4;
+                                src[1*step]= (sums[1] + sums[3] + 2*src[1*step])>>4;
+                                src[2*step]= (sums[2] + sums[4] + 2*src[2*step])>>4;
+                                src[3*step]= (sums[3] + sums[5] + 2*src[3*step])>>4;
+                                src[4*step]= (sums[4] + sums[6] + 2*src[4*step])>>4;
+                                src[5*step]= (sums[5] + sums[7] + 2*src[5*step])>>4;
+                                src[6*step]= (sums[6] + sums[8] + 2*src[6*step])>>4;
+                                src[7*step]= (sums[7] + sums[9] + 2*src[7*step])>>4;
+                        }
+                }else{
+                        const int middleEnergy= 5*(src[4*step] - src[3*step]) + 2*(src[2*step] - src[5*step]);
+
+                        if(ABS(middleEnergy) < 8*QP)
+                        {
+                                const int q=(src[3*step] - src[4*step])/2;
+                                const int leftEnergy=  5*(src[2*step] - src[1*step]) + 2*(src[0*step] - src[3*step]);
+                                const int rightEnergy= 5*(src[6*step] - src[5*step]) + 2*(src[4*step] - src[7*step]);
+
+                                int d= ABS(middleEnergy) - MIN( ABS(leftEnergy), ABS(rightEnergy) );
+                                d= MAX(d, 0);
+
+                                d= (5*d + 32) >> 6;
+                                d*= SIGN(-middleEnergy);
+
+                                if(q>0)
+                                {
+                                        d= d<0 ? 0 : d;
+                                        d= d>q ? q : d;
+                                }
+                                else
+                                {
+                                        d= d>0 ? 0 : d;
+                                        d= d<q ? q : d;
+                                }
+
+                                src[3*step]-= d;
+                                src[4*step]+= d;
+                        }
+                }
+
+                src += stride;
+        }
+/*if(step==16){
+    STOP_TIMER("step16")
+}else{
+    STOP_TIMER("stepX")
+}*/
+}
+
+//Note: we have C, MMX, MMX2, 3DNOW version there is no 3DNOW+MMX2 one
+//Plain C versions
+#if !defined (HAVE_MMX) || defined (RUNTIME_CPUDETECT)
+#define COMPILE_C
+#endif
+
+#ifdef ARCH_POWERPC
+#ifdef HAVE_ALTIVEC
+#define COMPILE_ALTIVEC
+#endif //HAVE_ALTIVEC
+#endif //ARCH_POWERPC
+
+#undef HAVE_MMX
+#undef HAVE_MMX2
+#undef HAVE_3DNOW
+#undef HAVE_ALTIVEC
+
+#ifdef ARCH_POWERPC
+#ifdef COMPILE_ALTIVEC
+#undef RENAME
+#define HAVE_ALTIVEC
+#define RENAME(a) a ## _altivec
+#include "postprocess_altivec_template.c"
+#include "postprocess_template.c"
+#endif
+#endif //ARCH_POWERPC
+
Index: ffmpeg-0.cvs20070307/libpostproc/postprocess_template.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libpostproc/postprocess_template.c	2007-03-28 16:17:35.000000000 +0200
+++ ffmpeg-0.cvs20070307/libpostproc/postprocess_template.c	2007-03-28 16:17:39.000000000 +0200
@@ -3181,7 +3181,10 @@
 }
 #endif //HAVE_MMX
 
-static void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
+#ifndef HAVE_ALTIVEC
+static
+#endif
+void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
         QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c);
 
 /**
@@ -3384,7 +3387,10 @@
 /**
  * Filters array of bytes (Y or U or V values)
  */
-static void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
+#ifndef HAVE_ALTIVEC
+static
+#endif
+void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
         QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c2)
 {
         PPContext __attribute__((aligned(8))) c= *c2; //copy to stack for faster access
Index: ffmpeg-0.cvs20070307/libswscale/swscale.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libswscale/swscale.c	2007-03-07 14:37:07.000000000 +0100
+++ ffmpeg-0.cvs20070307/libswscale/swscale.c	2007-03-28 16:17:39.000000000 +0200
@@ -849,7 +849,10 @@
 #undef RENAME
 #define HAVE_ALTIVEC
 #define RENAME(a) a ## _altivec
-#include "swscale_template.c"
+//#include "swscale_template.c"
+int yv12toyuy2_unscaled_altivec(SwsContext *, uint8_t*[], int[], int, int, uint8_t*[], int[]);
+int yv12touyvy_unscaled_altivec(SwsContext *, uint8_t*[], int[], int, int, uint8_t*[], int[]);
+int RENAME(swScale)(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY, int srcSliceH, uint8_t* dst[], int dstStride[]);
 #endif
 #endif //ARCH_POWERPC
 
@@ -2241,8 +2244,8 @@
 				srcFilter->chrV, dstFilter->chrV, c->param);
 
 #ifdef HAVE_ALTIVEC
-		c->vYCoeffsBank = av_malloc(sizeof (vector signed short)*c->vLumFilterSize*c->dstH);
-		c->vCCoeffsBank = av_malloc(sizeof (vector signed short)*c->vChrFilterSize*c->chrDstH);
+		c->vYCoeffsBank = av_malloc(16*c->vLumFilterSize*c->dstH);
+		c->vCCoeffsBank = av_malloc(16*c->vChrFilterSize*c->chrDstH);
 
 		for (i=0;i<c->vLumFilterSize*c->dstH;i++) {
                   int j;
Index: ffmpeg-0.cvs20070307/libswscale/swscale_altivec.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ ffmpeg-0.cvs20070307/libswscale/swscale_altivec.c	2007-03-28 16:17:39.000000000 +0200
@@ -0,0 +1,729 @@
+/*
+ * Copyright (C) 2001-2003 Michael Niedermayer <michaelni@gmx.at>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ *
+ * the C code (not assembly, mmx, ...) of this file can be used
+ * under the LGPL license too
+ */
+
+/*
+  supported Input formats: YV12, I420/IYUV, YUY2, UYVY, BGR32, BGR24, BGR16, BGR15, RGB32, RGB24, Y8/Y800, YVU9/IF09, PAL8
+  supported output formats: YV12, I420/IYUV, YUY2, UYVY, {BGR,RGB}{1,4,8,15,16,24,32}, Y8/Y800, YVU9/IF09
+  {BGR,RGB}{1,4,8,15,16} support dithering
+  
+  unscaled special converters (YV12=I420=IYUV, Y800=Y8)
+  YV12 -> {BGR,RGB}{1,4,8,15,16,24,32}
+  x -> x
+  YUV9 -> YV12
+  YUV9/YV12 -> Y800
+  Y800 -> YUV9/YV12
+  BGR24 -> BGR32 & RGB24 -> RGB32
+  BGR32 -> BGR24 & RGB32 -> RGB24
+  BGR15 -> BGR16
+*/
+
+/* 
+tested special converters (most are tested actually but i didnt write it down ...)
+ YV12 -> BGR16
+ YV12 -> YV12
+ BGR15 -> BGR16
+ BGR16 -> BGR16
+ YVU9 -> YV12
+
+untested special converters
+  YV12/I420 -> BGR15/BGR24/BGR32 (its the yuv2rgb stuff, so it should be ok)
+  YV12/I420 -> YV12/I420
+  YUY2/BGR15/BGR24/BGR32/RGB24/RGB32 -> same format
+  BGR24 -> BGR32 & RGB24 -> RGB32
+  BGR32 -> BGR24 & RGB32 -> RGB24
+  BGR24 -> YV12
+*/
+
+#include <inttypes.h>
+#include <string.h>
+#include <math.h>
+#include <stdio.h>
+#include <unistd.h>
+#include "config.h"
+#include <assert.h>
+#ifdef HAVE_SYS_MMAN_H
+#include <sys/mman.h>
+#if defined(MAP_ANON) && !defined(MAP_ANONYMOUS)
+#define MAP_ANONYMOUS MAP_ANON
+#endif
+#endif
+#include "swscale.h"
+#include "swscale_internal.h"
+#include "x86_cpu.h"
+#include "bswap.h"
+#include "rgb2rgb.h"
+#ifdef USE_FASTMEMCPY
+#include "libvo/fastmemcpy.h"
+#endif
+
+#undef MOVNTQ
+#undef PAVGB
+
+//#undef HAVE_MMX2
+//#define HAVE_3DNOW
+//#undef HAVE_MMX
+//#undef ARCH_X86
+//#define WORDS_BIGENDIAN
+#define DITHER1XBPP
+
+#define FAST_BGR2YV12 // use 7 bit coeffs instead of 15bit
+
+#define RET 0xC3 //near return opcode for X86
+
+#ifdef MP_DEBUG
+#define ASSERT(x) assert(x);
+#else
+#define ASSERT(x) ;
+#endif
+
+#ifdef M_PI
+#define PI M_PI
+#else
+#define PI 3.14159265358979323846
+#endif
+
+#define isSupportedIn(x)  ((x)==PIX_FMT_YUV420P || (x)==PIX_FMT_YUYV422 || (x)==PIX_FMT_UYVY422\
+			|| (x)==PIX_FMT_RGB32|| (x)==PIX_FMT_BGR24|| (x)==PIX_FMT_BGR565|| (x)==PIX_FMT_BGR555\
+			|| (x)==PIX_FMT_BGR32|| (x)==PIX_FMT_RGB24|| (x)==PIX_FMT_RGB565|| (x)==PIX_FMT_RGB555\
+			|| (x)==PIX_FMT_GRAY8 || (x)==PIX_FMT_YUV410P\
+			|| (x)==PIX_FMT_GRAY16BE || (x)==PIX_FMT_GRAY16LE\
+			|| (x)==PIX_FMT_YUV444P || (x)==PIX_FMT_YUV422P || (x)==PIX_FMT_YUV411P\
+			|| (x)==PIX_FMT_PAL8 || (x)==PIX_FMT_BGR8 || (x)==PIX_FMT_RGB8\
+                        || (x)==PIX_FMT_BGR4_BYTE  || (x)==PIX_FMT_RGB4_BYTE)
+#define isSupportedOut(x) ((x)==PIX_FMT_YUV420P || (x)==PIX_FMT_YUYV422 || (x)==PIX_FMT_UYVY422\
+			|| (x)==PIX_FMT_YUV444P || (x)==PIX_FMT_YUV422P || (x)==PIX_FMT_YUV411P\
+			|| isRGB(x) || isBGR(x)\
+			|| (x)==PIX_FMT_NV12 || (x)==PIX_FMT_NV21\
+			|| (x)==PIX_FMT_GRAY16BE || (x)==PIX_FMT_GRAY16LE\
+			|| (x)==PIX_FMT_GRAY8 || (x)==PIX_FMT_YUV410P)
+#define isPacked(x)    ((x)==PIX_FMT_YUYV422 || (x)==PIX_FMT_UYVY422 ||isRGB(x) || isBGR(x))
+
+#define RGB2YUV_SHIFT 16
+#define BY ((int)( 0.098*(1<<RGB2YUV_SHIFT)+0.5))
+#define BV ((int)(-0.071*(1<<RGB2YUV_SHIFT)+0.5))
+#define BU ((int)( 0.439*(1<<RGB2YUV_SHIFT)+0.5))
+#define GY ((int)( 0.504*(1<<RGB2YUV_SHIFT)+0.5))
+#define GV ((int)(-0.368*(1<<RGB2YUV_SHIFT)+0.5))
+#define GU ((int)(-0.291*(1<<RGB2YUV_SHIFT)+0.5))
+#define RY ((int)( 0.257*(1<<RGB2YUV_SHIFT)+0.5))
+#define RV ((int)( 0.439*(1<<RGB2YUV_SHIFT)+0.5))
+#define RU ((int)(-0.148*(1<<RGB2YUV_SHIFT)+0.5))
+
+extern const int32_t Inverse_Table_6_9[8][4];
+
+/*
+NOTES
+Special versions: fast Y 1:1 scaling (no interpolation in y direction)
+
+TODO
+more intelligent missalignment avoidance for the horizontal scaler
+write special vertical cubic upscale version
+Optimize C code (yv12 / minmax)
+add support for packed pixel yuv input & output
+add support for Y8 output
+optimize bgr24 & bgr32
+add BGR4 output support
+write special BGR->BGR scaler
+*/
+
+#if defined(ARCH_X86) && defined (CONFIG_GPL)
+static uint64_t attribute_used __attribute__((aligned(8))) bF8=       0xF8F8F8F8F8F8F8F8LL;
+static uint64_t attribute_used __attribute__((aligned(8))) bFC=       0xFCFCFCFCFCFCFCFCLL;
+static uint64_t __attribute__((aligned(8))) w10=       0x0010001000100010LL;
+static uint64_t attribute_used __attribute__((aligned(8))) w02=       0x0002000200020002LL;
+static uint64_t attribute_used __attribute__((aligned(8))) bm00001111=0x00000000FFFFFFFFLL;
+static uint64_t attribute_used __attribute__((aligned(8))) bm00000111=0x0000000000FFFFFFLL;
+static uint64_t attribute_used __attribute__((aligned(8))) bm11111000=0xFFFFFFFFFF000000LL;
+static uint64_t attribute_used __attribute__((aligned(8))) bm01010101=0x00FF00FF00FF00FFLL;
+
+static volatile uint64_t attribute_used __attribute__((aligned(8))) b5Dither;
+static volatile uint64_t attribute_used __attribute__((aligned(8))) g5Dither;
+static volatile uint64_t attribute_used __attribute__((aligned(8))) g6Dither;
+static volatile uint64_t attribute_used __attribute__((aligned(8))) r5Dither;
+
+static uint64_t __attribute__((aligned(8))) dither4[2]={
+	0x0103010301030103LL,
+	0x0200020002000200LL,};
+
+static uint64_t __attribute__((aligned(8))) dither8[2]={
+	0x0602060206020602LL,
+	0x0004000400040004LL,};
+
+static uint64_t __attribute__((aligned(8))) b16Mask=   0x001F001F001F001FLL;
+static uint64_t attribute_used __attribute__((aligned(8))) g16Mask=   0x07E007E007E007E0LL;
+static uint64_t attribute_used __attribute__((aligned(8))) r16Mask=   0xF800F800F800F800LL;
+static uint64_t __attribute__((aligned(8))) b15Mask=   0x001F001F001F001FLL;
+static uint64_t attribute_used __attribute__((aligned(8))) g15Mask=   0x03E003E003E003E0LL;
+static uint64_t attribute_used __attribute__((aligned(8))) r15Mask=   0x7C007C007C007C00LL;
+
+static uint64_t attribute_used __attribute__((aligned(8))) M24A=   0x00FF0000FF0000FFLL;
+static uint64_t attribute_used __attribute__((aligned(8))) M24B=   0xFF0000FF0000FF00LL;
+static uint64_t attribute_used __attribute__((aligned(8))) M24C=   0x0000FF0000FF0000LL;
+
+#ifdef FAST_BGR2YV12
+static const uint64_t bgr2YCoeff  attribute_used __attribute__((aligned(8))) = 0x000000210041000DULL;
+static const uint64_t bgr2UCoeff  attribute_used __attribute__((aligned(8))) = 0x0000FFEEFFDC0038ULL;
+static const uint64_t bgr2VCoeff  attribute_used __attribute__((aligned(8))) = 0x00000038FFD2FFF8ULL;
+#else
+static const uint64_t bgr2YCoeff  attribute_used __attribute__((aligned(8))) = 0x000020E540830C8BULL;
+static const uint64_t bgr2UCoeff  attribute_used __attribute__((aligned(8))) = 0x0000ED0FDAC23831ULL;
+static const uint64_t bgr2VCoeff  attribute_used __attribute__((aligned(8))) = 0x00003831D0E6F6EAULL;
+#endif /* FAST_BGR2YV12 */
+static const uint64_t bgr2YOffset attribute_used __attribute__((aligned(8))) = 0x1010101010101010ULL;
+static const uint64_t bgr2UVOffset attribute_used __attribute__((aligned(8)))= 0x8080808080808080ULL;
+static const uint64_t w1111       attribute_used __attribute__((aligned(8))) = 0x0001000100010001ULL;
+#endif /* defined(ARCH_X86) */
+
+// clipping helper table for C implementations:
+static unsigned char clip_table[768];
+
+static SwsVector *sws_getConvVec(SwsVector *a, SwsVector *b);
+		  
+extern const uint8_t dither_2x2_4[2][8];
+extern const uint8_t dither_2x2_8[2][8];
+extern const uint8_t dither_8x8_32[8][8];
+extern const uint8_t dither_8x8_73[8][8];
+extern const uint8_t dither_8x8_220[8][8];
+
+static const char * sws_context_to_name(void * ptr) {
+    return "swscaler";
+}
+
+static AVClass sws_context_class = { "SWScaler", sws_context_to_name, NULL };
+
+#if defined(ARCH_X86) && defined (CONFIG_GPL)
+void in_asm_used_var_warning_killer()
+{
+ volatile int i= bF8+bFC+w10+
+ bm00001111+bm00000111+bm11111000+b16Mask+g16Mask+r16Mask+b15Mask+g15Mask+r15Mask+
+ M24A+M24B+M24C+w02 + b5Dither+g5Dither+r5Dither+g6Dither+dither4[0]+dither8[0]+bm01010101;
+ if(i) i=0;
+}
+#endif
+
+static inline void yuv2yuvXinC(int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+				    int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+				    uint8_t *dest, uint8_t *uDest, uint8_t *vDest, int dstW, int chrDstW)
+{
+	//FIXME Optimize (just quickly writen not opti..)
+	int i;
+	for(i=0; i<dstW; i++)
+	{
+		int val=1<<18;
+		int j;
+		for(j=0; j<lumFilterSize; j++)
+			val += lumSrc[j][i] * lumFilter[j];
+
+		dest[i]= av_clip_uint8(val>>19);
+	}
+
+	if(uDest != NULL)
+		for(i=0; i<chrDstW; i++)
+		{
+			int u=1<<18;
+			int v=1<<18;
+			int j;
+			for(j=0; j<chrFilterSize; j++)
+			{
+				u += chrSrc[j][i] * chrFilter[j];
+				v += chrSrc[j][i + 2048] * chrFilter[j];
+			}
+
+			uDest[i]= av_clip_uint8(u>>19);
+			vDest[i]= av_clip_uint8(v>>19);
+		}
+}
+
+static inline void yuv2nv12XinC(int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+				int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+				uint8_t *dest, uint8_t *uDest, int dstW, int chrDstW, int dstFormat)
+{
+	//FIXME Optimize (just quickly writen not opti..)
+	int i;
+	for(i=0; i<dstW; i++)
+	{
+		int val=1<<18;
+		int j;
+		for(j=0; j<lumFilterSize; j++)
+			val += lumSrc[j][i] * lumFilter[j];
+
+		dest[i]= av_clip_uint8(val>>19);
+	}
+
+	if(uDest == NULL)
+		return;
+
+	if(dstFormat == PIX_FMT_NV12)
+		for(i=0; i<chrDstW; i++)
+		{
+			int u=1<<18;
+			int v=1<<18;
+			int j;
+			for(j=0; j<chrFilterSize; j++)
+			{
+				u += chrSrc[j][i] * chrFilter[j];
+				v += chrSrc[j][i + 2048] * chrFilter[j];
+			}
+
+			uDest[2*i]= av_clip_uint8(u>>19);
+			uDest[2*i+1]= av_clip_uint8(v>>19);
+		}
+	else
+		for(i=0; i<chrDstW; i++)
+		{
+			int u=1<<18;
+			int v=1<<18;
+			int j;
+			for(j=0; j<chrFilterSize; j++)
+			{
+				u += chrSrc[j][i] * chrFilter[j];
+				v += chrSrc[j][i + 2048] * chrFilter[j];
+			}
+
+			uDest[2*i]= av_clip_uint8(v>>19);
+			uDest[2*i+1]= av_clip_uint8(u>>19);
+		}
+}
+
+#define YSCALE_YUV_2_PACKEDX_C(type) \
+		for(i=0; i<(dstW>>1); i++){\
+			int j;\
+			int Y1=1<<18;\
+			int Y2=1<<18;\
+			int U=1<<18;\
+			int V=1<<18;\
+			type attribute_unused *r, *b, *g;\
+			const int i2= 2*i;\
+			\
+			for(j=0; j<lumFilterSize; j++)\
+			{\
+				Y1 += lumSrc[j][i2] * lumFilter[j];\
+				Y2 += lumSrc[j][i2+1] * lumFilter[j];\
+			}\
+			for(j=0; j<chrFilterSize; j++)\
+			{\
+				U += chrSrc[j][i] * chrFilter[j];\
+				V += chrSrc[j][i+2048] * chrFilter[j];\
+			}\
+			Y1>>=19;\
+			Y2>>=19;\
+			U >>=19;\
+			V >>=19;\
+			if((Y1|Y2|U|V)&256)\
+			{\
+				if(Y1>255)   Y1=255;\
+				else if(Y1<0)Y1=0;\
+				if(Y2>255)   Y2=255;\
+				else if(Y2<0)Y2=0;\
+				if(U>255)    U=255;\
+				else if(U<0) U=0;\
+				if(V>255)    V=255;\
+				else if(V<0) V=0;\
+			}
+                        
+#define YSCALE_YUV_2_RGBX_C(type) \
+			YSCALE_YUV_2_PACKEDX_C(type)\
+			r = (type *)c->table_rV[V];\
+			g = (type *)(c->table_gU[U] + c->table_gV[V]);\
+			b = (type *)c->table_bU[U];\
+
+#define YSCALE_YUV_2_PACKED2_C \
+		for(i=0; i<(dstW>>1); i++){\
+			const int i2= 2*i;\
+			int Y1= (buf0[i2  ]*yalpha1+buf1[i2  ]*yalpha)>>19;\
+			int Y2= (buf0[i2+1]*yalpha1+buf1[i2+1]*yalpha)>>19;\
+			int U= (uvbuf0[i     ]*uvalpha1+uvbuf1[i     ]*uvalpha)>>19;\
+			int V= (uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)>>19;\
+
+#define YSCALE_YUV_2_RGB2_C(type) \
+			YSCALE_YUV_2_PACKED2_C\
+			type *r, *b, *g;\
+			r = (type *)c->table_rV[V];\
+			g = (type *)(c->table_gU[U] + c->table_gV[V]);\
+			b = (type *)c->table_bU[U];\
+
+#define YSCALE_YUV_2_PACKED1_C \
+		for(i=0; i<(dstW>>1); i++){\
+			const int i2= 2*i;\
+			int Y1= buf0[i2  ]>>7;\
+			int Y2= buf0[i2+1]>>7;\
+			int U= (uvbuf1[i     ])>>7;\
+			int V= (uvbuf1[i+2048])>>7;\
+
+#define YSCALE_YUV_2_RGB1_C(type) \
+			YSCALE_YUV_2_PACKED1_C\
+			type *r, *b, *g;\
+			r = (type *)c->table_rV[V];\
+			g = (type *)(c->table_gU[U] + c->table_gV[V]);\
+			b = (type *)c->table_bU[U];\
+
+#define YSCALE_YUV_2_PACKED1B_C \
+		for(i=0; i<(dstW>>1); i++){\
+			const int i2= 2*i;\
+			int Y1= buf0[i2  ]>>7;\
+			int Y2= buf0[i2+1]>>7;\
+			int U= (uvbuf0[i     ] + uvbuf1[i     ])>>8;\
+			int V= (uvbuf0[i+2048] + uvbuf1[i+2048])>>8;\
+
+#define YSCALE_YUV_2_RGB1B_C(type) \
+			YSCALE_YUV_2_PACKED1B_C\
+			type *r, *b, *g;\
+			r = (type *)c->table_rV[V];\
+			g = (type *)(c->table_gU[U] + c->table_gV[V]);\
+			b = (type *)c->table_bU[U];\
+
+#define YSCALE_YUV_2_ANYRGB_C(func, func2)\
+	switch(c->dstFormat)\
+	{\
+	case PIX_FMT_RGB32:\
+	case PIX_FMT_BGR32:\
+		func(uint32_t)\
+			((uint32_t*)dest)[i2+0]= r[Y1] + g[Y1] + b[Y1];\
+			((uint32_t*)dest)[i2+1]= r[Y2] + g[Y2] + b[Y2];\
+		}		\
+		break;\
+	case PIX_FMT_RGB24:\
+		func(uint8_t)\
+			((uint8_t*)dest)[0]= r[Y1];\
+			((uint8_t*)dest)[1]= g[Y1];\
+			((uint8_t*)dest)[2]= b[Y1];\
+			((uint8_t*)dest)[3]= r[Y2];\
+			((uint8_t*)dest)[4]= g[Y2];\
+			((uint8_t*)dest)[5]= b[Y2];\
+			dest+=6;\
+		}\
+		break;\
+	case PIX_FMT_BGR24:\
+		func(uint8_t)\
+			((uint8_t*)dest)[0]= b[Y1];\
+			((uint8_t*)dest)[1]= g[Y1];\
+			((uint8_t*)dest)[2]= r[Y1];\
+			((uint8_t*)dest)[3]= b[Y2];\
+			((uint8_t*)dest)[4]= g[Y2];\
+			((uint8_t*)dest)[5]= r[Y2];\
+			dest+=6;\
+		}\
+		break;\
+	case PIX_FMT_RGB565:\
+	case PIX_FMT_BGR565:\
+		{\
+			const int dr1= dither_2x2_8[y&1    ][0];\
+			const int dg1= dither_2x2_4[y&1    ][0];\
+			const int db1= dither_2x2_8[(y&1)^1][0];\
+			const int dr2= dither_2x2_8[y&1    ][1];\
+			const int dg2= dither_2x2_4[y&1    ][1];\
+			const int db2= dither_2x2_8[(y&1)^1][1];\
+			func(uint16_t)\
+				((uint16_t*)dest)[i2+0]= r[Y1+dr1] + g[Y1+dg1] + b[Y1+db1];\
+				((uint16_t*)dest)[i2+1]= r[Y2+dr2] + g[Y2+dg2] + b[Y2+db2];\
+			}\
+		}\
+		break;\
+	case PIX_FMT_RGB555:\
+	case PIX_FMT_BGR555:\
+		{\
+			const int dr1= dither_2x2_8[y&1    ][0];\
+			const int dg1= dither_2x2_8[y&1    ][1];\
+			const int db1= dither_2x2_8[(y&1)^1][0];\
+			const int dr2= dither_2x2_8[y&1    ][1];\
+			const int dg2= dither_2x2_8[y&1    ][0];\
+			const int db2= dither_2x2_8[(y&1)^1][1];\
+			func(uint16_t)\
+				((uint16_t*)dest)[i2+0]= r[Y1+dr1] + g[Y1+dg1] + b[Y1+db1];\
+				((uint16_t*)dest)[i2+1]= r[Y2+dr2] + g[Y2+dg2] + b[Y2+db2];\
+			}\
+		}\
+		break;\
+	case PIX_FMT_RGB8:\
+	case PIX_FMT_BGR8:\
+		{\
+			const uint8_t * const d64= dither_8x8_73[y&7];\
+			const uint8_t * const d32= dither_8x8_32[y&7];\
+			func(uint8_t)\
+				((uint8_t*)dest)[i2+0]= r[Y1+d32[(i2+0)&7]] + g[Y1+d32[(i2+0)&7]] + b[Y1+d64[(i2+0)&7]];\
+				((uint8_t*)dest)[i2+1]= r[Y2+d32[(i2+1)&7]] + g[Y2+d32[(i2+1)&7]] + b[Y2+d64[(i2+1)&7]];\
+			}\
+		}\
+		break;\
+	case PIX_FMT_RGB4:\
+	case PIX_FMT_BGR4:\
+		{\
+			const uint8_t * const d64= dither_8x8_73 [y&7];\
+			const uint8_t * const d128=dither_8x8_220[y&7];\
+			func(uint8_t)\
+				((uint8_t*)dest)[i]= r[Y1+d128[(i2+0)&7]] + g[Y1+d64[(i2+0)&7]] + b[Y1+d128[(i2+0)&7]]\
+				                 + ((r[Y2+d128[(i2+1)&7]] + g[Y2+d64[(i2+1)&7]] + b[Y2+d128[(i2+1)&7]])<<4);\
+			}\
+		}\
+		break;\
+	case PIX_FMT_RGB4_BYTE:\
+	case PIX_FMT_BGR4_BYTE:\
+		{\
+			const uint8_t * const d64= dither_8x8_73 [y&7];\
+			const uint8_t * const d128=dither_8x8_220[y&7];\
+			func(uint8_t)\
+				((uint8_t*)dest)[i2+0]= r[Y1+d128[(i2+0)&7]] + g[Y1+d64[(i2+0)&7]] + b[Y1+d128[(i2+0)&7]];\
+				((uint8_t*)dest)[i2+1]= r[Y2+d128[(i2+1)&7]] + g[Y2+d64[(i2+1)&7]] + b[Y2+d128[(i2+1)&7]];\
+			}\
+		}\
+		break;\
+	case PIX_FMT_MONOBLACK:\
+		{\
+			const uint8_t * const d128=dither_8x8_220[y&7];\
+			uint8_t *g= c->table_gU[128] + c->table_gV[128];\
+			for(i=0; i<dstW-7; i+=8){\
+				int acc;\
+				acc =       g[((buf0[i  ]*yalpha1+buf1[i  ]*yalpha)>>19) + d128[0]];\
+				acc+= acc + g[((buf0[i+1]*yalpha1+buf1[i+1]*yalpha)>>19) + d128[1]];\
+				acc+= acc + g[((buf0[i+2]*yalpha1+buf1[i+2]*yalpha)>>19) + d128[2]];\
+				acc+= acc + g[((buf0[i+3]*yalpha1+buf1[i+3]*yalpha)>>19) + d128[3]];\
+				acc+= acc + g[((buf0[i+4]*yalpha1+buf1[i+4]*yalpha)>>19) + d128[4]];\
+				acc+= acc + g[((buf0[i+5]*yalpha1+buf1[i+5]*yalpha)>>19) + d128[5]];\
+				acc+= acc + g[((buf0[i+6]*yalpha1+buf1[i+6]*yalpha)>>19) + d128[6]];\
+				acc+= acc + g[((buf0[i+7]*yalpha1+buf1[i+7]*yalpha)>>19) + d128[7]];\
+				((uint8_t*)dest)[0]= acc;\
+				dest++;\
+			}\
+\
+/*\
+((uint8_t*)dest)-= dstW>>4;\
+{\
+			int acc=0;\
+			int left=0;\
+			static int top[1024];\
+			static int last_new[1024][1024];\
+			static int last_in3[1024][1024];\
+			static int drift[1024][1024];\
+			int topLeft=0;\
+			int shift=0;\
+			int count=0;\
+			const uint8_t * const d128=dither_8x8_220[y&7];\
+			int error_new=0;\
+			int error_in3=0;\
+			int f=0;\
+			\
+			for(i=dstW>>1; i<dstW; i++){\
+				int in= ((buf0[i  ]*yalpha1+buf1[i  ]*yalpha)>>19);\
+				int in2 = (76309 * (in - 16) + 32768) >> 16;\
+				int in3 = (in2 < 0) ? 0 : ((in2 > 255) ? 255 : in2);\
+				int old= (left*7 + topLeft + top[i]*5 + top[i+1]*3)/20 + in3\
+					+ (last_new[y][i] - in3)*f/256;\
+				int new= old> 128 ? 255 : 0;\
+\
+				error_new+= FFABS(last_new[y][i] - new);\
+				error_in3+= FFABS(last_in3[y][i] - in3);\
+				f= error_new - error_in3*4;\
+				if(f<0) f=0;\
+				if(f>256) f=256;\
+\
+				topLeft= top[i];\
+				left= top[i]= old - new;\
+				last_new[y][i]= new;\
+				last_in3[y][i]= in3;\
+\
+				acc+= acc + (new&1);\
+				if((i&7)==6){\
+					((uint8_t*)dest)[0]= acc;\
+					((uint8_t*)dest)++;\
+				}\
+			}\
+}\
+*/\
+		}\
+		break;\
+	case PIX_FMT_YUYV422:\
+		func2\
+			((uint8_t*)dest)[2*i2+0]= Y1;\
+			((uint8_t*)dest)[2*i2+1]= U;\
+			((uint8_t*)dest)[2*i2+2]= Y2;\
+			((uint8_t*)dest)[2*i2+3]= V;\
+		}		\
+		break;\
+	case PIX_FMT_UYVY422:\
+		func2\
+			((uint8_t*)dest)[2*i2+0]= U;\
+			((uint8_t*)dest)[2*i2+1]= Y1;\
+			((uint8_t*)dest)[2*i2+2]= V;\
+			((uint8_t*)dest)[2*i2+3]= Y2;\
+		}		\
+		break;\
+	}\
+
+
+static inline void yuv2packedXinC(SwsContext *c, int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+				    int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+				    uint8_t *dest, int dstW, int y)
+{
+	int i;
+	switch(c->dstFormat)
+	{
+	case PIX_FMT_BGR32:
+	case PIX_FMT_RGB32:
+		YSCALE_YUV_2_RGBX_C(uint32_t)
+			((uint32_t*)dest)[i2+0]= r[Y1] + g[Y1] + b[Y1];
+			((uint32_t*)dest)[i2+1]= r[Y2] + g[Y2] + b[Y2];
+		}
+		break;
+	case PIX_FMT_RGB24:
+		YSCALE_YUV_2_RGBX_C(uint8_t)
+			((uint8_t*)dest)[0]= r[Y1];
+			((uint8_t*)dest)[1]= g[Y1];
+			((uint8_t*)dest)[2]= b[Y1];
+			((uint8_t*)dest)[3]= r[Y2];
+			((uint8_t*)dest)[4]= g[Y2];
+			((uint8_t*)dest)[5]= b[Y2];
+			dest+=6;
+		}
+		break;
+	case PIX_FMT_BGR24:
+		YSCALE_YUV_2_RGBX_C(uint8_t)
+			((uint8_t*)dest)[0]= b[Y1];
+			((uint8_t*)dest)[1]= g[Y1];
+			((uint8_t*)dest)[2]= r[Y1];
+			((uint8_t*)dest)[3]= b[Y2];
+			((uint8_t*)dest)[4]= g[Y2];
+			((uint8_t*)dest)[5]= r[Y2];
+			dest+=6;
+		}
+		break;
+	case PIX_FMT_RGB565:
+	case PIX_FMT_BGR565:
+		{
+			const int dr1= dither_2x2_8[y&1    ][0];
+			const int dg1= dither_2x2_4[y&1    ][0];
+			const int db1= dither_2x2_8[(y&1)^1][0];
+			const int dr2= dither_2x2_8[y&1    ][1];
+			const int dg2= dither_2x2_4[y&1    ][1];
+			const int db2= dither_2x2_8[(y&1)^1][1];
+			YSCALE_YUV_2_RGBX_C(uint16_t)
+				((uint16_t*)dest)[i2+0]= r[Y1+dr1] + g[Y1+dg1] + b[Y1+db1];
+				((uint16_t*)dest)[i2+1]= r[Y2+dr2] + g[Y2+dg2] + b[Y2+db2];
+			}
+		}
+		break;
+	case PIX_FMT_RGB555:
+	case PIX_FMT_BGR555:
+		{
+			const int dr1= dither_2x2_8[y&1    ][0];
+			const int dg1= dither_2x2_8[y&1    ][1];
+			const int db1= dither_2x2_8[(y&1)^1][0];
+			const int dr2= dither_2x2_8[y&1    ][1];
+			const int dg2= dither_2x2_8[y&1    ][0];
+			const int db2= dither_2x2_8[(y&1)^1][1];
+			YSCALE_YUV_2_RGBX_C(uint16_t)
+				((uint16_t*)dest)[i2+0]= r[Y1+dr1] + g[Y1+dg1] + b[Y1+db1];
+				((uint16_t*)dest)[i2+1]= r[Y2+dr2] + g[Y2+dg2] + b[Y2+db2];
+			}
+		}
+		break;
+	case PIX_FMT_RGB8:
+	case PIX_FMT_BGR8:
+		{
+			const uint8_t * const d64= dither_8x8_73[y&7];
+			const uint8_t * const d32= dither_8x8_32[y&7];
+			YSCALE_YUV_2_RGBX_C(uint8_t)
+				((uint8_t*)dest)[i2+0]= r[Y1+d32[(i2+0)&7]] + g[Y1+d32[(i2+0)&7]] + b[Y1+d64[(i2+0)&7]];
+				((uint8_t*)dest)[i2+1]= r[Y2+d32[(i2+1)&7]] + g[Y2+d32[(i2+1)&7]] + b[Y2+d64[(i2+1)&7]];
+			}
+		}
+		break;
+	case PIX_FMT_RGB4:
+	case PIX_FMT_BGR4:
+		{
+			const uint8_t * const d64= dither_8x8_73 [y&7];
+			const uint8_t * const d128=dither_8x8_220[y&7];
+			YSCALE_YUV_2_RGBX_C(uint8_t)
+				((uint8_t*)dest)[i]= r[Y1+d128[(i2+0)&7]] + g[Y1+d64[(i2+0)&7]] + b[Y1+d128[(i2+0)&7]]
+				                  +((r[Y2+d128[(i2+1)&7]] + g[Y2+d64[(i2+1)&7]] + b[Y2+d128[(i2+1)&7]])<<4);
+			}
+		}
+		break;
+	case PIX_FMT_RGB4_BYTE:
+	case PIX_FMT_BGR4_BYTE:
+		{
+			const uint8_t * const d64= dither_8x8_73 [y&7];
+			const uint8_t * const d128=dither_8x8_220[y&7];
+			YSCALE_YUV_2_RGBX_C(uint8_t)
+				((uint8_t*)dest)[i2+0]= r[Y1+d128[(i2+0)&7]] + g[Y1+d64[(i2+0)&7]] + b[Y1+d128[(i2+0)&7]];
+				((uint8_t*)dest)[i2+1]= r[Y2+d128[(i2+1)&7]] + g[Y2+d64[(i2+1)&7]] + b[Y2+d128[(i2+1)&7]];
+			}
+		}
+		break;
+	case PIX_FMT_MONOBLACK:
+		{
+			const uint8_t * const d128=dither_8x8_220[y&7];
+			uint8_t *g= c->table_gU[128] + c->table_gV[128];
+			int acc=0;
+			for(i=0; i<dstW-1; i+=2){
+				int j;
+				int Y1=1<<18;
+				int Y2=1<<18;
+
+				for(j=0; j<lumFilterSize; j++)
+				{
+					Y1 += lumSrc[j][i] * lumFilter[j];
+					Y2 += lumSrc[j][i+1] * lumFilter[j];
+				}
+				Y1>>=19;
+				Y2>>=19;
+				if((Y1|Y2)&256)
+				{
+					if(Y1>255)   Y1=255;
+					else if(Y1<0)Y1=0;
+					if(Y2>255)   Y2=255;
+					else if(Y2<0)Y2=0;
+				}
+				acc+= acc + g[Y1+d128[(i+0)&7]];
+				acc+= acc + g[Y2+d128[(i+1)&7]];
+				if((i&7)==6){
+					((uint8_t*)dest)[0]= acc;
+					dest++;
+				}
+			}
+		}
+		break;
+	case PIX_FMT_YUYV422:
+		YSCALE_YUV_2_PACKEDX_C(void)
+			((uint8_t*)dest)[2*i2+0]= Y1;
+			((uint8_t*)dest)[2*i2+1]= U;
+			((uint8_t*)dest)[2*i2+2]= Y2;
+			((uint8_t*)dest)[2*i2+3]= V;
+		}
+                break;
+	case PIX_FMT_UYVY422:
+		YSCALE_YUV_2_PACKEDX_C(void)
+			((uint8_t*)dest)[2*i2+0]= U;
+			((uint8_t*)dest)[2*i2+1]= Y1;
+			((uint8_t*)dest)[2*i2+2]= V;
+			((uint8_t*)dest)[2*i2+3]= Y2;
+		}
+                break;
+	}
+}
+
+
+#undef RENAME
+#define HAVE_ALTIVEC
+#define RENAME(a) a ## _altivec
+#include "swscale_template.c"
+
Index: ffmpeg-0.cvs20070307/libswscale/swscale_altivec_template.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libswscale/swscale_altivec_template.c	2007-03-07 14:37:07.000000000 +0100
+++ ffmpeg-0.cvs20070307/libswscale/swscale_altivec_template.c	2007-03-28 16:17:39.000000000 +0200
@@ -390,7 +390,7 @@
   }
 }
 
-static inline int yv12toyuy2_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
+int yv12toyuy2_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
      int srcSliceH, uint8_t* dstParam[], int dstStride_a[]) {
   uint8_t *dst=dstParam[0] + dstStride_a[0]*srcSliceY;
   // yv12toyuy2( src[0],src[1],src[2],dst,c->srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
@@ -469,7 +469,7 @@
   return srcSliceH;
 }
 
-static inline int yv12touyvy_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
+int yv12touyvy_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
      int srcSliceH, uint8_t* dstParam[], int dstStride_a[]) {
   uint8_t *dst=dstParam[0] + dstStride_a[0]*srcSliceY;
   // yv12toyuy2( src[0],src[1],src[2],dst,c->srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
Index: ffmpeg-0.cvs20070307/libswscale/swscale_internal.h
===================================================================
--- ffmpeg-0.cvs20070307.orig/libswscale/swscale_internal.h	2007-03-07 14:37:07.000000000 +0100
+++ ffmpeg-0.cvs20070307/libswscale/swscale_internal.h	2007-03-28 16:17:39.000000000 +0200
@@ -21,9 +21,11 @@
 #ifndef SWSCALE_INTERNAL_H
 #define SWSCALE_INTERNAL_H
 
+#ifdef ABI_ALTIVEC
 #ifdef HAVE_ALTIVEC_H
 #include <altivec.h>
 #endif
+#endif
 
 #include "avutil.h"
 
@@ -150,15 +152,20 @@
 	uint64_t v_temp       __attribute__((aligned(8)));
 
 #ifdef HAVE_ALTIVEC
+#ifdef ABI_ALTIVEC
+#define VEC(x) vector x
+#else
+#define VEC(x) union { char c[16]; }
+#endif
 
-  vector signed short   CY;
-  vector signed short   CRV;
-  vector signed short   CBU;
-  vector signed short   CGU;
-  vector signed short   CGV;
-  vector signed short   OY;
-  vector unsigned short CSHIFT;
-  vector signed short *vYCoeffsBank, *vCCoeffsBank;
+  VEC(signed short)   CY;
+  VEC(signed short)   CRV;
+  VEC(signed short)   CBU;
+  VEC(signed short)   CGU;
+  VEC(signed short)   CGV;
+  VEC(signed short)   OY;
+  VEC(unsigned short) CSHIFT;
+  VEC(signed short) *vYCoeffsBank, *vCCoeffsBank;
 
 #endif
 
Index: ffmpeg-0.cvs20070307/libswscale/Makefile
===================================================================
--- ffmpeg-0.cvs20070307.orig/libswscale/Makefile	2007-03-07 14:37:07.000000000 +0100
+++ ffmpeg-0.cvs20070307/libswscale/Makefile	2007-03-28 16:17:39.000000000 +0200
@@ -9,8 +9,10 @@
 
 OBJS= swscale.o rgb2rgb.o
 
-OBJS-$(TARGET_ALTIVEC)     +=  yuv2rgb_altivec.o
+OBJS-$(TARGET_ALTIVEC)     +=  swscale_altivec.o yuv2rgb_altivec.o
 OBJS-$(CONFIG_GPL)         +=  yuv2rgb.o
+swscale_altivec.o: CFLAGS+= $(ALTIVECFLAGS) -DABI_ALTIVEC
+yuv2rgb_altivec.o: CFLAGS+= $(ALTIVECFLAGS) -DABI_ALTIVEC
 
 HEADERS = swscale.h rgb2rgb.h
 
Index: ffmpeg-0.cvs20070307/libswscale/swscale_template.c
===================================================================
--- ffmpeg-0.cvs20070307.orig/libswscale/swscale_template.c	2007-03-07 14:37:07.000000000 +0100
+++ ffmpeg-0.cvs20070307/libswscale/swscale_template.c	2007-03-28 16:17:39.000000000 +0200
@@ -2936,7 +2936,10 @@
    }
 }
 
-static int RENAME(swScale)(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
+#ifndef HAVE_ALTIVEC
+static
+#endif
+int RENAME(swScale)(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
              int srcSliceH, uint8_t* dst[], int dstStride[]){
 
 	/* load a few things into local vars to make the code more readable? and faster */
